{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Function Calling Test\n",
    "\n",
    "We will be testing out the intelligent JSON generator in the GPT API. You can find out more about it here: https://platform.openai.com/docs/guides/gpt/chat-completions-api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import openai\n",
    "import tiktoken\n",
    "import json\n",
    "import requests\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the OpenAI API key from the .env file\n",
    "load_dotenv()\n",
    "secret_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The following transcript of a lab experiement has text with start and end times of when they were said in a video. Edit the transcript into a clean and concise lab procedure that would appear in a lab report that contains the start and end times in each of the bullet points: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're going to find [0.0-1.0]\n",
      " the weight of an empty 25 graduate 25 millimeter graduated cylinder so you want to turn the scale on [1.0-10.0]\n",
      " if it doesn't read zero [10.0-12.0]\n",
      " when you first turn it on tear or zero is a button that you want to push to make sure that it reads zero when nothing is on the scale [12.0-24.0]\n",
      " okay then you can put your graduated cylinder on and record the mass of your graduated cylinder [24.0-32.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Obtaining raw transcript w/ time\n",
    "cwd = os.getcwd()\n",
    "\n",
    "\n",
    "# read in transcript txt file\n",
    "transcript = \"\"\n",
    "with open(\"data/gpt_test_transcript_time.txt\", \"r\") as file:\n",
    "    transcript = file.read()\n",
    "\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling Testing\n",
    "\n",
    "I attempted to use function calling, but this does not actually achieve what we actually want from API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "# Utility functions from OpenAI Cookbook\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, function_call=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    if function_call is not None:\n",
    "        json_data.update({\"function_call\": function_call})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n",
    "def pretty_print_conversation(messages):\n",
    "    role_to_color = {\n",
    "        \"system\": \"red\",\n",
    "        \"user\": \"green\",\n",
    "        \"assistant\": \"blue\",\n",
    "        \"function\": \"magenta\",\n",
    "    }\n",
    "    formatted_messages = []\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"system\":\n",
    "            formatted_messages.append(f\"system: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"user\":\n",
    "            formatted_messages.append(f\"user: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
    "            formatted_messages.append(f\"assistant: {message['function_call']}\\n\")\n",
    "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
    "            formatted_messages.append(f\"assistant: {message['content']}\\n\")\n",
    "        elif message[\"role\"] == \"function\":\n",
    "            formatted_messages.append(f\"function ({message['name']}): {message['content']}\\n\")\n",
    "    for formatted_message in formatted_messages:\n",
    "        print(\n",
    "            colored(\n",
    "                formatted_message,\n",
    "                role_to_color[messages[formatted_messages.index(formatted_message)][\"role\"]],\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define JSON file format\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"get_single_lab_instructions\",\n",
    "        \"description\": \"Converts transcript of a lab experiment into a clean, consice, and time stamped lab report\",\n",
    "        \"parameters\":{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\":{\n",
    "                \"transcript\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Raw transcript with respective time stamps, e.g. I'll walk over here to grab this flask [12.0-13.5]\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"transcript\"]\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_lab_summary\",\n",
    "        \"description\": \"Generate a short summary of what we are accomplishing in the lab described in the transcript\",\n",
    "        \"parameters\":{\n",
    "            \"type\": \"object\",\n",
    "            \"properties\":{\n",
    "                \"transcript\":{\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Raw transcript with respective time stamps, e.g. I'll walk over here to grab this flask [12.0-13.5]\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"transcript\"]\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"format\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"location\", \"format\"],\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'function_call': {'name': 'get_single_lab_instructions',\n",
       "  'arguments': '{\\n\"transcript\": \"x\"\\n}'}}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": \"From the following lab transcript, generate a clean and concise lab procedure that would appear in a lab report: x \"})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"Generate a short lab report summary from the following transcript:\" + transcript})\n",
    "# messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
    "\n",
    "chat_response = chat_completion_request(\n",
    "    messages, functions=functions\n",
    ")\n",
    "\n",
    "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
    "messages.append(assistant_message)\n",
    "assistant_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'assistant',\n",
       " 'content': None,\n",
       " 'function_call': {'name': 'get_single_lab_instructions',\n",
       "  'arguments': '{\\n  \"transcript\": \"we\\'re going to find [0.0-1.0] the weight of an empty 25 graduate 25 millimeter graduated cylinder so you want to turn the scale on [1.0-10.0] if it doesn\\'t read zero [10.0-12.0] when you first turn it on tear or zero is a button that you want to push to make sure that it reads zero when nothing is on the scale [12.0-24.0] okay then you can put your graduated cylinder on and record the mass of your graduated cylinder [24.0-32.0]\"\\n}'}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom JSON Generator\n",
    "\n",
    "Welp let's give it a go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def properReformat(raw_response):\n",
    "\n",
    "    # Split the string by newlines\n",
    "    lines = raw_response.split('\\n')\n",
    "\n",
    "    # Remove empty lines\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "    # Format as a numerical list\n",
    "    formatted_lines = [f'{line}' for index, line in enumerate(lines)]\n",
    "\n",
    "    # Join the formatted lines with newlines\n",
    "    formatted_string = '\\n'.join(formatted_lines)\n",
    "\n",
    "    return formatted_string\n",
    "\n",
    "def properReformatv2(raw_response):\n",
    "    # Remove leading and trailing whitespace\n",
    "    raw_response = raw_response.strip()\n",
    "\n",
    "    # Remove the initial \"Procedure:\" string\n",
    "    raw_response = raw_response.replace(\"Procedure:\", \"\")\n",
    "\n",
    "    # Split the input string into individual steps\n",
    "    steps = raw_response.split(\"•\")\n",
    "\n",
    "    # Process each step to extract the content and time stamps\n",
    "    procedure = []\n",
    "    for step in steps:\n",
    "        # Remove leading and trailing whitespace\n",
    "        step = step.strip()\n",
    "\n",
    "        # Extract the content and time stamps using regex\n",
    "        import re\n",
    "        pattern = r\"^(.*?)\\((.*?)\\-(.*?)\\)$\"\n",
    "        match = re.match(pattern, step)\n",
    "        if match:\n",
    "            content = match.group(1).strip()\n",
    "            start_time = match.group(2).strip()\n",
    "            end_time = match.group(3).strip()\n",
    "\n",
    "            # Create a step object\n",
    "            step_obj = {\n",
    "                \"step\": content,\n",
    "                \"start_time\": start_time,\n",
    "                \"end_time\": end_time\n",
    "            }\n",
    "\n",
    "            # Append the step object to the procedure list\n",
    "            procedure.append(step_obj)\n",
    "\n",
    "    # Create the final JSON structure\n",
    "    json_data = {\"procedure\": procedure}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7aseXAGfDJIQEY9u2NfrWIYlRDVsi at 0x2baa7bb31f0> JSON: {\n",
       "  \"id\": \"cmpl-7aseXAGfDJIQEY9u2NfrWIYlRDVsi\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1689024249,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\nProcedure:\\n\\u2022 Turn on the scale (0.0-1.0)\\n\\u2022 Press the zero button if the scale does not read zero (10.0-12.0)\\n\\u2022 Place the empty 25 millimeter graduated cylinder on the scale (12.0-24.0)\\n\\u2022 Record the mass of the graduated cylinder (24.0-32.0)\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 184,\n",
       "    \"completion_tokens\": 82,\n",
       "    \"total_tokens\": 266\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = \"text-davinci-003\"\n",
    "\n",
    "#count tokens to figure out a good max_tokens value\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "encoding = tiktoken.encoding_for_model(model)\n",
    "num_tokens = len(encoding.encode(transcript))\n",
    "\n",
    "response_preset_output = openai.Completion.create(\n",
    "  model=model,\n",
    "  prompt= prompt + transcript,\n",
    "  temperature=0.2, #in range (0,2), higher = more creative\n",
    "  max_tokens=num_tokens,\n",
    ")\n",
    "\n",
    "response_preset_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProcedure:\\n• Turn on the scale (0.0-1.0)\\n• Press the zero button if the scale does not read zero (10.0-12.0)\\n• Place the empty 25 millimeter graduated cylinder on the scale (12.0-24.0)\\n• Record the mass of the graduated cylinder (24.0-32.0)'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_response_preset = response_preset_output.get('choices')[0].get('text')\n",
    "raw_response_preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procedure:\n",
      "• Turn on the scale (0.0-1.0)\n",
      "• Press the zero button if the scale does not read zero (10.0-12.0)\n",
      "• Place the empty 25 millimeter graduated cylinder on the scale (12.0-24.0)\n",
      "• Record the mass of the graduated cylinder (24.0-32.0)\n"
     ]
    }
   ],
   "source": [
    "response_preset = properReformat(raw_response_preset)\n",
    "print(response_preset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sst_transcription_2",
   "language": "python",
   "name": "sst_transcription_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
